{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q3"
      ],
      "metadata": {
        "id": "NgQDc4v3TfsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   We will develop a NER system specific to the category of names of the top 1000 movie titles from IMDB.\n",
        "\n",
        "*   We will evaluate the system on a collection of text likely to contain instances of these named entities."
      ],
      "metadata": {
        "id": "r3BJFGcYT-LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnwTjXyH5Xls",
        "outputId": "60c09729-4549-4530-985b-9c9c12f015e8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import math\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import brown, movie_reviews\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3DlFdNPT95l",
        "outputId": "a6851ad4-f467-4fa5-9004-0935284369d8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_1000_list():\n",
        "    \"\"\"\n",
        "    Function to extract movie titles from a IMDB-top-1000.csv file.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of unique titles of the top 1000 movies\n",
        "    \"\"\"\n",
        "    with open(\"./data/IMDB-top-1000.csv\", 'r') as file:\n",
        "        csvreader = csv.reader(file)\n",
        "        collected_titles = []\n",
        "        for row in csvreader:\n",
        "            raw_title = row[1].strip()\n",
        "            # so that no duplicates is collected\n",
        "            if raw_title not in collected_titles:\n",
        "                collected_titles.append(raw_title.split())\n",
        "    return collected_titles[1:]"
      ],
      "metadata": {
        "id": "5hHLacEwzP3q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_BIO(_tokens, _NE):\n",
        "    \"\"\"\n",
        "    Generates BIO (Beginning, Inside, Outside) tags for movie titles in the given tokens.\n",
        "\n",
        "    Args:\n",
        "        _tokens (list): List of tokens representing words in a sentence.\n",
        "        _NE (list): List of named entities, where each entity is represented as a list of tokens.\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples containing tokens and their corresponding BIO tags.\n",
        "\n",
        "    Comments:\n",
        "        - This function searches for movie titles in the tokens and labels them using BIO notation.\n",
        "        - A movie title is considered to be a named entity, where the first word is labeled as 'B-MOV'\n",
        "          (Beginning of a movie title) and subsequent words are labeled as 'I-MOV' (Inside a movie title).\n",
        "        - Non-movie title tokens are labeled as 'O' (Outside any named entity).\n",
        "        - The function iterates through each token in the tokens list, searching for matches in the named entity list.\n",
        "          If a match is found, the corresponding tokens are labeled accordingly in the BIO format.\n",
        "        - It returns a list of tuples, each containing a token and its corresponding BIO tag.\n",
        "    \"\"\"\n",
        "\n",
        "    # MOV: movie title\n",
        "    # beginning of a movie title is tagged with \"B-MOV\"\n",
        "    # middle/end of a movie title is tagged \"I-MOV\"\n",
        "\n",
        "    BIO_for_samples = []\n",
        "    sample_size = len(_tokens)\n",
        "\n",
        "    current_i = 0\n",
        "\n",
        "    # search through each sample\n",
        "    for i in range(sample_size):\n",
        "\n",
        "        if i != current_i: continue\n",
        "\n",
        "        # search each named entity for each sample\n",
        "        has_match = False\n",
        "        match_index = -1\n",
        "        for j in range(len(_NE)):\n",
        "\n",
        "            # if the first word is a match\n",
        "            if _NE[j][0] == _tokens[i]:\n",
        "\n",
        "                # if the matched movie title does not\n",
        "                # exceed the length of the sample\n",
        "                if i + len(_NE[j]) < sample_size:\n",
        "\n",
        "                    # check if all other components match\n",
        "                    no_match = False\n",
        "                    for k in range(1, len(_NE[j])):\n",
        "                        if _NE[j][k] != _tokens[i + k]:\n",
        "                            no_match = True\n",
        "                            break\n",
        "\n",
        "                    # full match found\n",
        "                    if not no_match:\n",
        "                        has_match = True\n",
        "                        match_index = j\n",
        "                        break\n",
        "\n",
        "        # Done searching through the Named Entity List\n",
        "        if has_match:\n",
        "            BIO_for_samples.append((_tokens[i], 'B-MOV'))\n",
        "            for j in range(1, len(_NE[match_index])):\n",
        "                BIO_for_samples.append((_tokens[i + j], 'I-MOV'))\n",
        "            current_i += len(_NE[match_index])\n",
        "        else:\n",
        "            BIO_for_samples.append((_tokens[i], 'O'))\n",
        "            current_i += 1\n",
        "\n",
        "    return BIO_for_samples"
      ],
      "metadata": {
        "id": "Mem-wyqU1z0f"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't change this cell\n",
        "def print_BIO_res(_BIO):\n",
        "    for i in range(len(_BIO)):\n",
        "        if _BIO[i][1] == 'B-MOV':\n",
        "            for j in range(i - 7, i + 7):\n",
        "                if _BIO[j][1] == 'O':\n",
        "                    print(_BIO[j][0], end=\" \")\n",
        "                else:\n",
        "                    print(_BIO[j], end=\" \")\n",
        "            print(\"\")"
      ],
      "metadata": {
        "id": "1UA4HbNA5Fj6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't change this cell\n",
        "def get_data_from_file(_fn):\n",
        "    with open(_fn, 'r') as file:\n",
        "        data = file.read().replace('\\n', ' ')\n",
        "    return data"
      ],
      "metadata": {
        "id": "vrQqvG947sb4"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles_top_1000 = get_top_1000_list()\n",
        "\n",
        "# get text data from a text file\n",
        "data = get_data_from_file(\"data/article-about-a-genre.txt\")\n",
        "# tokenize text data\n",
        "tokens = word_tokenize(data)\n",
        "# tag with BIO using the IMDB top 1000 movie title list\n",
        "BIO = label_BIO(tokens, titles_top_1000)\n",
        "\n",
        "# B-MOV marks the beginning of a known movie title\n",
        "# I-MOV marks the middle/end of a known movie title\n",
        "# O for all other words\n",
        "print_BIO_res(BIO)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdzyVGJ25IBc",
        "outputId": "3ed59dd5-3e29-4b9a-ac15-4388bc6102c1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ten Rings is shaping up to overtake ('Black', 'B-MOV') Widow as the biggest film of \n",
            ". With films like Chan ’ s ('Rush', 'B-MOV') Hour ( 1998 ) and Shanghai \n",
            "to find its way into hits like ('The', 'B-MOV') ('Matrix', 'I-MOV') ( 1999 ) and Kill \n",
            "the trend . Jet Li ’ s ('Hero', 'B-MOV') ( 2002 ) and Fearless ( \n",
            "comedies Shaolin Soccer ( 2001 ) and ('Kung', 'B-MOV') ('Fu', 'I-MOV') ('Hustle', 'I-MOV') ( 2004 ) , \n",
            ") , and Donnie Yen ’ s ('Ip', 'B-MOV') ('Man', 'I-MOV') ( 2008 ) . Shang-Chi \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_CiMfcXg5Py2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}